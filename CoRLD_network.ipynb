{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5b9f3-a5df-4c92-9ecb-ab5bc66a9ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44f39e45-3024-4ec2-9495-0d94c56789bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import PathLike\n",
    "from pathlib import Path\n",
    "import random\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os, glob\n",
    "import sys\n",
    "\n",
    "import yaml\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.nn.functional as nnf\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR, ReduceLROnPlateau\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from PIL import Image\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a289b98d-37fb-4447-87d1-088f59fec2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_folder = r\"...\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "label_mapping = {\n",
    "    \"CN\": 0,\n",
    "    \"AD\": 1,\n",
    "    \"EMCI\": 2,\n",
    "    \"LMCI\": 3\n",
    "}\n",
    "\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith(\".nii.gz\")]\n",
    "\n",
    "for filename in tqdm(file_list, desc=\"Processing Files\", unit=\"file\"):\n",
    "    for label_key, label_value in label_mapping.items():\n",
    "        if f\"_{label_key}.\" in filename:\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            try:\n",
    "                img = nib.load(file_path)\n",
    "                data = img.get_fdata()\n",
    "                if len(data.shape) != 2:\n",
    "                    continue\n",
    "                images.append(data)\n",
    "                labels.append(label_value)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {filename}: {e}\")\n",
    "            break\n",
    "\n",
    "images_array = np.stack(images)\n",
    "labels_array = np.array(labels)\n",
    "\n",
    "print(\"Images array shape:\", images_array.shape)\n",
    "print(\"Labels array shape:\", labels_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3348ef01-0732-44ae-8265-fe933db18705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_per_class_and_average(images_array, labels_array, samples_per_class=10):\n",
    "    unique_labels = np.unique(labels_array)\n",
    "    all_sampled_indices = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        class_indices = np.where(labels_array == label)[0]\n",
    "        \n",
    "        if len(class_indices) < samples_per_class:\n",
    "            print(f\"Warning: Class {label} has only {len(class_indices)} samples\")\n",
    "            n_samples = len(class_indices)\n",
    "        else:\n",
    "            n_samples = samples_per_class\n",
    "        \n",
    "        class_sampled_indices = np.random.choice(class_indices, size=n_samples, replace=False)\n",
    "        all_sampled_indices.extend(class_sampled_indices)\n",
    "    \n",
    "    all_sampled_indices = np.array(all_sampled_indices)\n",
    "    \n",
    "    sampled_images = images_array[all_sampled_indices]\n",
    "    sampled_labels = labels_array[all_sampled_indices]\n",
    "    \n",
    "    average_image = np.mean(sampled_images, axis=0)\n",
    "    \n",
    "    return average_image, all_sampled_indices, sampled_labels\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "average_image, sampled_indices, sampled_labels = sample_per_class_and_average(images_array, labels_array)\n",
    "\n",
    "print(f\"\\nTotal number of sampled images: {len(sampled_indices)}\")\n",
    "print(f\"Average image shape: {average_image.shape}\")\n",
    "\n",
    "print(f\"\\nAverage image stats:\")\n",
    "print(f\"  Min: {average_image.min():.3f}\")\n",
    "print(f\"  Max: {average_image.max():.3f}\")\n",
    "print(f\"  Mean: {average_image.mean():.3f}\")\n",
    "print(f\"  Std: {average_image.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96fc92-917c-4ec6-80e0-2efbe0079bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, registered_img_path, average_image, transform=None):\n",
    "        images = (images - images.min()) / (images.max() - images.min())\n",
    "        self.images = torch.FloatTensor(images)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        self.transform = transform\n",
    "        \n",
    "        average_image = (average_image - average_image.min()) / (average_image.max() - average_image.min())\n",
    "        self.average_image = torch.FloatTensor(average_image)\n",
    "        \n",
    "        registered_img = nib.load(registered_img_path).get_fdata()\n",
    "        registered_img = (registered_img - registered_img.min()) / (registered_img.max() - registered_img.min())\n",
    "        self.registered_middle_slice = registered_img[:, :, registered_img.shape[2] // 2]\n",
    "        self.registered_middle_slice = torch.FloatTensor(self.registered_middle_slice)\n",
    "\n",
    "        # Define fixed volume paths for each class\n",
    "        self.fixed_volume_paths = {\n",
    "            0: '...',\n",
    "            1: '...',\n",
    "            2: '...',\n",
    "            3: '...'\n",
    "        }\n",
    "        \n",
    "        self.fixed_middle_slices = {}\n",
    "        for label, path in self.fixed_volume_paths.items():\n",
    "            try:\n",
    "                print(f\"\\nProcessing file for label {label}: {path}\")\n",
    "                fixed_volume = nib.load(path).get_fdata()\n",
    "                print(f\"Shape of fixed volume for label {label}: {fixed_volume.shape}\")\n",
    "                \n",
    "                if len(fixed_volume.shape) < 3:\n",
    "                    raise ValueError(f\"Volume for label {label} has less than 3 dimensions: {fixed_volume.shape}\")\n",
    "                \n",
    "                fixed_volume = (fixed_volume - fixed_volume.min()) / (fixed_volume.max() - fixed_volume.min())\n",
    "                middle_slice = fixed_volume[:, :, fixed_volume.shape[2] // 2]\n",
    "                self.fixed_middle_slices[label] = torch.FloatTensor(middle_slice)\n",
    "                print(f\"Successfully processed label {label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing label {label}: {str(e)}\")\n",
    "                print(f\"File path: {path}\")\n",
    "                raise  \n",
    "        \n",
    "        if self.transform:\n",
    "            self.registered_middle_slice = self.transform(self.registered_middle_slice.unsqueeze(0))\n",
    "            self.average_image = self.transform(self.average_image.unsqueeze(0))\n",
    "            for label in self.fixed_middle_slices:\n",
    "                self.fixed_middle_slices[label] = self.transform(self.fixed_middle_slices[label].unsqueeze(0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        registered_image = self.registered_middle_slice\n",
    "        average_image = self.average_image\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        fixed_image = self.fixed_middle_slices[label.item()]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image.unsqueeze(0))\n",
    "        else:\n",
    "            image = image.unsqueeze(0)\n",
    "            \n",
    "        return (image.squeeze(0), \n",
    "                registered_image.squeeze(0), \n",
    "                average_image.squeeze(0),\n",
    "                fixed_image.squeeze(0),\n",
    "                label)\n",
    "\n",
    "def create_stratified_splits(images_array, labels_array, registered_img_path, average_image, batch_size=32, num_workers=4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  \n",
    "    ])\n",
    "\n",
    "    train_images, temp_images, train_labels, temp_labels = train_test_split(\n",
    "        images_array,\n",
    "        labels_array,\n",
    "        train_size=0.7,\n",
    "        stratify=labels_array,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    val_images, test_images, val_labels, test_labels = train_test_split(\n",
    "        temp_images,\n",
    "        temp_labels,\n",
    "        train_size=0.5,\n",
    "        stratify=temp_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = CustomDataset(train_images, train_labels, registered_img_path, average_image, transform=transform)\n",
    "    val_dataset = CustomDataset(val_images, val_labels, registered_img_path, average_image, transform=transform)\n",
    "    test_dataset = CustomDataset(test_images, test_labels, registered_img_path, average_image, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nDetailed class distribution in splits:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    train_dist = np.bincount(train_labels)\n",
    "    val_dist = np.bincount(val_labels)\n",
    "    test_dist = np.bincount(test_labels)\n",
    "    \n",
    "    print(f\"{'Class':<6} {'Train':<20} {'Validation':<20} {'Test':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for class_idx in range(len(train_dist)):\n",
    "        train_percent = (train_dist[class_idx] / len(train_labels)) * 100\n",
    "        val_percent = (val_dist[class_idx] / len(val_labels)) * 100\n",
    "        test_percent = (test_dist[class_idx] / len(test_labels)) * 100\n",
    "        \n",
    "        print(f\"{class_idx:<6} {train_dist[class_idx]:>4} ({train_percent:>5.1f}%)  {val_dist[class_idx]:>6} ({val_percent:>5.1f}%)  {test_dist[class_idx]:>6} ({test_percent:>5.1f}%)\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = create_stratified_splits(\n",
    "    images_array,\n",
    "    labels_array,\n",
    "    registered_img_path='...',\n",
    "    average_image=average_image,\n",
    "    batch_size=512,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "for batch_img, batch_reg_img, batch_avg_img, batch_fixed_img, batch_labels in train_loader:\n",
    "    print(\"Regular images shape:\", batch_img.shape)\n",
    "    print(\"Regular images range:\", batch_img.min().item(), \"to\", batch_img.max().item())\n",
    "\n",
    "    print(\"Registered image shape:\", batch_reg_img.shape)\n",
    "    print(\"Registered image range:\", batch_reg_img.min().item(), \"to\", batch_reg_img.max().item())\n",
    "\n",
    "    print(\"Average image shape:\", batch_avg_img.shape)\n",
    "    print(\"Average image range:\", batch_avg_img.min().item(), \"to\", batch_avg_img.max().item())\n",
    "\n",
    "    print(\"Fixed image shape:\", batch_fixed_img.shape)\n",
    "    print(\"Fixed image range:\", batch_fixed_img.min().item(), \"to\", batch_fixed_img.max().item())\n",
    "\n",
    "    print(\"Labels shape:\", batch_labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67231651-cc46-4586-802d-6cf314a320ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_batch(batch_img, batch_reg_img, batch_labels, num_samples=8):\n",
    "    if torch.is_tensor(batch_img):\n",
    "        batch_img = batch_img.numpy()\n",
    "    if torch.is_tensor(batch_reg_img):\n",
    "        batch_reg_img = batch_reg_img.numpy()\n",
    "    if torch.is_tensor(batch_labels):\n",
    "        batch_labels = batch_labels.numpy()\n",
    "    \n",
    "    batch_img = batch_img[:num_samples]\n",
    "    batch_reg_img = batch_reg_img[:num_samples]\n",
    "    batch_labels = batch_labels[:num_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 5))\n",
    "    plt.suptitle('Batch Visualization: Original Images (top) vs Registered Images (bottom)', fontsize=14)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        axes[0, i].imshow(batch_img[i], cmap='gray')\n",
    "        axes[0, i].axis('off')\n",
    "        axes[0, i].set_title(f'Label: {batch_labels[i]}')\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        axes[1, i].imshow(batch_reg_img[i], cmap='gray')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for batch_img, batch_reg_img, batch_avg_img, batch_fixed_img, batch_labels in train_loader:\n",
    "    visualize_batch(batch_img, batch_fixed_img, batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fab4a0e0-be91-4fce-a403-bed3a6139d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################Parameter Loading#######################\n",
    "def read_yaml(path):\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            file = edict(yaml.load(f, Loader=yaml.FullLoader))\n",
    "        return file\n",
    "    except:\n",
    "        print('NO FILE READ!')\n",
    "        return None\n",
    "para = read_yaml('./parameters.yml')\n",
    "\n",
    "xDim = 128 \n",
    "yDim = 128\n",
    "zDim = 1\n",
    "\n",
    "def loss_Reg(y_pred):\n",
    "        # For 3D reg\n",
    "        # dy = torch.abs(y_pred[:, :, 1:, :, :] - y_pred[:, :, :-1, :, :])\n",
    "        # dx = torch.abs(y_pred[:, :, :, 1:, :] - y_pred[:, :, :, :-1, :])\n",
    "        # dz = torch.abs(y_pred[:, :, :, :, 1:] - y_pred[:, :, :, :, :-1])\n",
    "        # dy = dy * dy\n",
    "        # dx = dx * dx\n",
    "        # dz = dz * dz\n",
    "        # d = torch.mean(dx) + torch.mean(dy) + torch.mean(dz)\n",
    "        # grad = d / 3.0\n",
    "\n",
    "        dy = torch.abs(y_pred[:, :, 1:, :] - y_pred[:, :, :-1, :])\n",
    "        dx = torch.abs(y_pred[:, :, :, 1:] - y_pred[:, :, :, :-1])\n",
    "\n",
    "        dy = dy * dy\n",
    "        dx = dx * dx\n",
    "        d = torch.mean(dx) + torch.mean(dy) \n",
    "        grad = d / 2.0\n",
    "        return grad\n",
    "    \n",
    "\n",
    "def jacobian_determinant(displacement):\n",
    "    \"\"\"\n",
    "    Calculate Jacobian determinant of the deformation field\n",
    "    displacement: tensor of shape [batch, 2, H, W]\n",
    "    \"\"\"\n",
    "    # Get x and y components of displacement field\n",
    "    disp_x = displacement[:, 0, :, :]  # [batch, H, W]\n",
    "    disp_y = displacement[:, 1, :, :]  # [batch, H, W]\n",
    "    \n",
    "    # Calculate gradients\n",
    "    dx_x = disp_x[:, :, 1:] - disp_x[:, :, :-1]  # partial_x of x displacement\n",
    "    dx_y = disp_y[:, :, 1:] - disp_y[:, :, :-1]  # partial_x of y displacement\n",
    "    dy_x = disp_x[:, 1:, :-1] - disp_x[:, :-1, :-1]  # partial_y of x displacement\n",
    "    dy_y = disp_y[:, 1:, :-1] - disp_y[:, :-1, :-1]  # partial_y of y displacement\n",
    "\n",
    "    # Calculate Jacobian determinant\n",
    "    det = (1 + dx_x[:, :-1, :]) * (1 + dy_y) - dx_y[:, :-1, :] * dy_x\n",
    "\n",
    "    return torch.mean((det - 1)**2)\n",
    "\n",
    "class NCCLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Normalized Cross Correlation loss.\n",
    "    Returns a loss that when minimized maximizes the NCC between the input and the template.\n",
    "    Zero is the perfect score.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(NCCLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # Compute means\n",
    "        pred_mean = pred.mean(dim=(-2, -1), keepdim=True)\n",
    "        target_mean = target.mean(dim=(-2, -1), keepdim=True)\n",
    "\n",
    "        # Compute normalized variables\n",
    "        pred_norm = pred - pred_mean\n",
    "        target_norm = target - target_mean\n",
    "\n",
    "        # Compute variances\n",
    "        pred_var = torch.sum(pred_norm ** 2, dim=(-2, -1), keepdim=True)\n",
    "        target_var = torch.sum(target_norm ** 2, dim=(-2, -1), keepdim=True)\n",
    "\n",
    "        # Compute cross correlation\n",
    "        cross_corr = torch.sum(pred_norm * target_norm, dim=(-2, -1), keepdim=True)\n",
    "\n",
    "        # Compute NCC\n",
    "        ncc = cross_corr / (torch.sqrt(pred_var) * torch.sqrt(target_var) + self.eps)\n",
    "\n",
    "        # Return loss (1 - NCC), bounded between 0 and 2\n",
    "        return 1 - ncc.mean()\n",
    "    \n",
    "from losses import MSE, Grad\n",
    "#################Network optimization########################\n",
    "from networks import DiffeoDense  \n",
    "net = []\n",
    "for i in range(3):\n",
    "    temp = DiffeoDense(inshape = (xDim,yDim),\n",
    "\t\t\t\t nb_unet_features= [[16, 32],[ 32, 32, 16, 16]],\n",
    "                 nb_unet_conv_per_level=1,\n",
    "                 int_steps=7,\n",
    "                 int_downsize=2,\n",
    "                 src_feats=1,\n",
    "                 trg_feats=1,\n",
    "                 unet_half_res= True)\n",
    "    net.append(temp)\n",
    "net = net[0].to(dev)\n",
    "\n",
    "if(para.model.loss == 'L2'):\n",
    "    criterion = nn.MSELoss()\n",
    "elif (para.model.loss == 'L1'):\n",
    "    criterion = nn.L1Loss()\n",
    "if(para.model.optimizer == 'Adam'):\n",
    "    optimizer = optim.Adam(net.parameters(), lr= para.solver.lr)\n",
    "elif (para.model.optimizer == 'SGD'):\n",
    "    optimizer = optim.SGD(net.parameters(), lr= para.solver.lr, momentum=0.9)\n",
    "if (para.model.scheduler == 'CosAn'):\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader), eta_min=0)\n",
    "    \n",
    "class SupConLoss(nn.Module):\n",
    "    \"\"\"Enhanced Supervised Contrastive Learning loss with diagnostics\"\"\"\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, features, labels):\n",
    "        if len(features.shape) > 2:\n",
    "            b, c, h, w = features.shape\n",
    "            features = features.view(b, c * h * w)\n",
    "            # print(f\"Reshaped features - shape: {features.shape}\")\n",
    "        \n",
    "        device = features.device\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Normalize features\n",
    "        features = F.normalize(features, dim=1)\n",
    "        # print(f\"Normalized features - min: {features.min():.4f}, \"\n",
    "        #       f\"max: {features.max():.4f}, mean: {features.mean():.4f}\")\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        sim_matrix = torch.matmul(features, features.T) / self.temperature\n",
    "        # print(f\"Similarity matrix - min: {sim_matrix.min():.4f}, \"\n",
    "        #       f\"max: {sim_matrix.max():.4f}, mean: {sim_matrix.mean():.4f}\")\n",
    "        \n",
    "        # Get positive pair mask\n",
    "        labels = labels.view(-1, 1)\n",
    "        mask = torch.eq(labels, labels.T).float().to(device)\n",
    "        \n",
    "        # Print positive pair statistics\n",
    "        pos_pairs = mask.sum().item() - batch_size  # subtract diagonal\n",
    "        total_possible = batch_size * (batch_size - 1)\n",
    "        # print(f\"Positive pairs: {pos_pairs}, Negative pairs: {total_possible - pos_pairs}\")\n",
    "        \n",
    "        # Mask out self-contrast cases\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size).view(-1, 1).to(device),\n",
    "            0\n",
    "        )\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # Compute log probabilities\n",
    "        exp_sim = torch.exp(sim_matrix) * logits_mask\n",
    "        log_prob = sim_matrix - torch.log(exp_sim.sum(1, keepdim=True))\n",
    "        \n",
    "        # Mean of log-probability over positive pairs\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / torch.clamp(mask.sum(1), min=1e-12)\n",
    "        \n",
    "        # Loss\n",
    "        loss = -mean_log_prob_pos.mean()\n",
    "        # print(f\"Contrastive loss: {loss.item():.4f}\")\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "class FeatureTransform(nn.Module):\n",
    "    \"\"\"Transform features to be more suitable for contrastive learning\"\"\"\n",
    "    def __init__(self, in_channels=32):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8, 8))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn(self.conv(x)))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # Handle any NaN values\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"Warning: Features contain NaN values. Replacing with zeros.\")\n",
    "            x = torch.nan_to_num(x, nan=0.0)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18f3f60-2e1c-4daf-b06d-f5ecaaf60907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize losses and feature transform\n",
    "criterion = NCCLoss()\n",
    "feature_transform = FeatureTransform().to(dev)\n",
    "con_criterion = SupConLoss(temperature=0.1)\n",
    "\n",
    "# Initialize optimizer with both networks\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': net.parameters()},\n",
    "    {'params': feature_transform.parameters()}\n",
    "], lr=para.solver.lr)\n",
    "\n",
    "training_losses = []\n",
    "reg_losses = []\n",
    "con_losses = []\n",
    "total_batches = len(train_loader) * para.solver.epochs\n",
    "\n",
    "# Initialize progress bar for the entire training process\n",
    "with tqdm(total=total_batches, desc=\"Training Progress\") as pbar:\n",
    "    for epoch in range(para.solver.epochs):\n",
    "        total = 0\n",
    "        total_reg = 0\n",
    "        total_con = 0\n",
    "        running_loss = 0.0\n",
    "        net.train()\n",
    "        feature_transform.train()\n",
    "        \n",
    "        for src_bch, _, _, tar_bch, labels in train_loader:\n",
    "            b, w, h = src_bch.shape\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Prepare input batches\n",
    "            src_bch = src_bch.reshape(b, 1, w, h).to(dev)\n",
    "            tar_bch = tar_bch.reshape(b, 1, w, h).to(dev)\n",
    "            labels = labels.to(dev)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = net(src_bch, tar_bch, registration=True)\n",
    "            \n",
    "            # Registration loss\n",
    "            reg_loss = criterion(pred[0], src_bch)\n",
    "            smoothness_loss = loss_Reg(pred[1])\n",
    "            \n",
    "            # Transform features and compute contrastive loss\n",
    "            transformed_features = feature_transform(pred[2])\n",
    "            con_loss = con_criterion(transformed_features, labels)\n",
    "            \n",
    "            # Combined loss with balanced weights\n",
    "            loss_total = reg_loss + smoothness_loss + 0.001 * con_loss\n",
    "            \n",
    "            # Gradient clipping before backward pass\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "            torch.nn.utils.clip_grad_norm_(feature_transform.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update running losses\n",
    "            running_loss += loss_total.item()\n",
    "            total += loss_total.item()\n",
    "            total_reg += reg_loss.item()\n",
    "            total_con += con_loss.item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                \"Total Loss\": f\"{loss_total.item():.4f}\",\n",
    "                \"Reg Loss\": f\"{reg_loss.item():.4f}\",\n",
    "                \"Con Loss\": f\"{con_loss.item():.4f}\"\n",
    "            })\n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Append epoch losses to lists\n",
    "        avg_loss = total / len(train_loader)\n",
    "        avg_reg_loss = total_reg / len(train_loader)\n",
    "        avg_con_loss = total_con / len(train_loader)\n",
    "        \n",
    "        training_losses.append(avg_loss)\n",
    "        reg_losses.append(avg_reg_loss)\n",
    "        con_losses.append(avg_con_loss)\n",
    "\n",
    "# Plot the training losses\n",
    "plt.figure(figsize=(15, 4))\n",
    "\n",
    "# Plot total loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, para.solver.epochs + 1), training_losses, marker='o', color='b', label='Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Training Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot registration loss\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(range(1, para.solver.epochs + 1), reg_losses, marker='o', color='g', label='Registration Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Registration Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot contrastive loss\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(range(1, para.solver.epochs + 1), con_losses, marker='o', color='r', label='Contrastive Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Contrastive Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the complete model\n",
    "torch.save(net, '...')\n",
    "torch.save(feature_transform, '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cc297-ce86-4ab5-a2ae-49e6d4ee8c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "class RawFeatureClassifier(nn.Module):\n",
    "    \"\"\"Classifier for raw features - original architecture\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "class TransformedFeatureClassifier(nn.Module):\n",
    "    \"\"\"Classifier for transformed features - more sophisticated architecture\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "def extract_features(loader, pretrained_net, feature_transform=None, device='cuda'):\n",
    "    \"\"\"Extract both raw and transformed features from loader\"\"\"\n",
    "    raw_features_list = []\n",
    "    transformed_features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    pretrained_net.eval()\n",
    "    if feature_transform is not None:\n",
    "        feature_transform.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_img, batch_reg_img, batch_avg_img, batch_const_img, batch_labels in tqdm(loader, desc=\"Extracting features\"):\n",
    "            # Move data to device\n",
    "            batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2]).to(device)\n",
    "            batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], batch_const_img.shape[2]).to(device)\n",
    "            \n",
    "            # Forward pass through pretrained network\n",
    "            pred = pretrained_net(batch_img, batch_const_img, registration=True)\n",
    "            raw_features = pred[2]\n",
    "            \n",
    "            # Flatten raw features\n",
    "            raw_features = raw_features.view(raw_features.size(0), -1)\n",
    "            raw_features_list.append(raw_features.cpu())\n",
    "            \n",
    "            # Get transformed features if feature_transform is provided\n",
    "            if feature_transform is not None:\n",
    "                transformed_features = feature_transform(pred[2])\n",
    "                transformed_features = transformed_features.view(transformed_features.size(0), -1)\n",
    "                transformed_features_list.append(transformed_features.cpu())\n",
    "            \n",
    "            labels_list.append(batch_labels)\n",
    "    \n",
    "    raw_features = torch.cat(raw_features_list, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "    \n",
    "    if feature_transform is not None:\n",
    "        transformed_features = torch.cat(transformed_features_list, dim=0)\n",
    "        return raw_features, transformed_features, labels\n",
    "    \n",
    "    return raw_features, labels\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_score=None):\n",
    "    \"\"\"Calculate metrics including AUC score when probability scores are provided\"\"\"\n",
    "    # Overall accuracy\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'per_class_acc': per_class_acc,\n",
    "        'conf_matrix': conf_matrix\n",
    "    }\n",
    "    \n",
    "    # Add AUC score if probability scores are provided\n",
    "    if y_score is not None:\n",
    "        try:\n",
    "            auc = roc_auc_score(y_true, y_score, multi_class='ovr')\n",
    "            metrics_dict['auc'] = auc\n",
    "        except:\n",
    "            metrics_dict['auc'] = None\n",
    "            print(\"Warning: Could not calculate AUC score\")\n",
    "    else:\n",
    "        metrics_dict['auc'] = None\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, save_path=None):\n",
    "    \"\"\"Plot and optionally save confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_classifier(pretrained_net, train_loader, val_loader, feature_transform=None, save_dir='./classifier_results', feature_type='raw'):\n",
    "    \"\"\"Train classifier using either raw or transformed features\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    if feature_transform is not None:\n",
    "        train_raw, train_transformed, train_labels = extract_features(train_loader, pretrained_net, feature_transform, device)\n",
    "        val_raw, val_transformed, val_labels = extract_features(val_loader, pretrained_net, feature_transform, device)\n",
    "        \n",
    "        # Select features based on feature_type\n",
    "        train_features = train_transformed if feature_type == 'transformed' else train_raw\n",
    "        val_features = val_transformed if feature_type == 'transformed' else val_raw\n",
    "    else:\n",
    "        train_features, train_labels = extract_features(train_loader, pretrained_net, None, device)\n",
    "        val_features, val_labels = extract_features(val_loader, pretrained_net, None, device)\n",
    "    \n",
    "    # Convert labels to long type\n",
    "    train_labels = train_labels.long()\n",
    "    val_labels = val_labels.long()\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_loader_clf = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader_clf = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model based on feature type\n",
    "    input_dim = train_features.shape[1]\n",
    "    hidden_dim = 256\n",
    "    num_classes = len(torch.unique(train_labels))\n",
    "    \n",
    "    if feature_type == 'transformed':\n",
    "        model = TransformedFeatureClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
    "        learning_rate = 0.000005\n",
    "    else:\n",
    "        model = RawFeatureClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
    "        learning_rate = 0.00005\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 300\n",
    "    early_stopping_patience = 30\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    # Training loop with progress bar\n",
    "    print(f\"Starting training with {feature_type} features...\")\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for features, labels in train_loader_clf:\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            features = features.float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader_clf:\n",
    "                features, labels = features.to(device), labels.to(device)\n",
    "                features = features.float()\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        train_loss = train_loss / len(train_loader_clf)\n",
    "        val_loss = val_loss / len(val_loader_clf)\n",
    "        train_acc = train_correct / total_train\n",
    "        val_acc = val_correct / total_val\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'val_loss': f'{val_loss:.4f}',\n",
    "            'train_acc': f'{train_acc:.4f}',\n",
    "            'val_acc': f'{val_acc:.4f}'\n",
    "        })\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Save best model\n",
    "    model_filename = f'best_feature_classifier_{feature_type}.pth'\n",
    "    torch.save(best_model_state, os.path.join(save_dir, model_filename))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Loss History ({feature_type} features)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title(f'Accuracy History ({feature_type} features)')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'training_history_{feature_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return model, best_model_state\n",
    "\n",
    "def test_classifier(pretrained_net, test_loader, model_state, feature_transform=None, feature_type='raw', save_dir='./classifier_results'):\n",
    "    \"\"\"Test the trained classifier with either raw or transformed features\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Extract test features\n",
    "    if feature_transform is not None:\n",
    "        test_raw, test_transformed, test_labels = extract_features(test_loader, pretrained_net, feature_transform, device)\n",
    "        test_features = test_transformed if feature_type == 'transformed' else test_raw\n",
    "    else:\n",
    "        test_features, test_labels = extract_features(test_loader, pretrained_net, None, device)\n",
    "    \n",
    "    test_labels = test_labels.long()\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    test_loader_clf = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Initialize appropriate model type\n",
    "    input_dim = test_features.shape[1]\n",
    "    hidden_dim = 256\n",
    "    num_classes = len(torch.unique(test_labels))\n",
    "    \n",
    "    if feature_type == 'transformed':\n",
    "        model = TransformedFeatureClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
    "    else:\n",
    "        model = RawFeatureClassifier(input_dim, hidden_dim, num_classes).to(device)\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(test_loader_clf, desc=f\"Testing ({feature_type} features)\"):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            features = features.float()\n",
    "            outputs = model(features)\n",
    "            scores = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_scores.extend(scores.cpu().numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_scores = np.array(all_scores)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(all_labels, all_preds, all_scores)\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics ({feature_type} features):\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    if metrics['auc'] is not None:\n",
    "        print(f\"AUC Score: {metrics['auc']:.4f}\")\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i, acc in enumerate(metrics['per_class_acc']):\n",
    "        print(f\"Class {i}: {acc:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(metrics['conf_matrix'], os.path.join(save_dir, f'confusion_matrix_{feature_type}.png'))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load models\n",
    "    pretrained_net = torch.load('...').to(device)\n",
    "    feature_transform = torch.load('...').to(device)\n",
    "    \n",
    "    save_dir = './saved_models'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Train and test with raw features\n",
    "    print(\"\\nTraining with raw features...\")\n",
    "    model_raw, state_raw = train_classifier(pretrained_net, train_loader, val_loader, \n",
    "                                          feature_transform=feature_transform, \n",
    "                                          feature_type='raw', \n",
    "                                          save_dir=save_dir)\n",
    "    \n",
    "    print(\"\\nTesting with raw features...\")\n",
    "    metrics_raw = test_classifier(pretrained_net, test_loader, state_raw, \n",
    "                                feature_transform=feature_transform, \n",
    "                                feature_type='raw', \n",
    "                                save_dir=save_dir)\n",
    "    \n",
    "    # Train and test with transformed features\n",
    "    print(\"\\nTraining with transformed features...\")\n",
    "    model_transformed, state_transformed = train_classifier(pretrained_net, train_loader, val_loader, \n",
    "                                                         feature_transform=feature_transform, \n",
    "                                                         feature_type='transformed', \n",
    "                                                         save_dir=save_dir)\n",
    "    \n",
    "    print(\"\\nTesting with transformed features...\")\n",
    "    metrics_transformed = test_classifier(pretrained_net, test_loader, state_transformed, \n",
    "                                       feature_transform=feature_transform, \n",
    "                                       feature_type='transformed', \n",
    "                                       save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756e564-afb3-441c-a196-ce56ca986cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(images, noise_level):\n",
    "    \"\"\"Add Gaussian noise to images\"\"\"\n",
    "    noise = torch.randn_like(images) * noise_level\n",
    "    noisy_images = images + noise\n",
    "    return torch.clamp(noisy_images, 0, 1)\n",
    "\n",
    "def test_noise_robustness(pretrained_net, feature_transform, test_loader, model_states, \n",
    "                         noise_levels=[0, 0.001, 0.005, 0.01, 0.05], save_dir='./classifier_results'):\n",
    "    \"\"\"Test model robustness against different levels of Gaussian noise for both raw and transformed features\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    pretrained_net.eval()\n",
    "    feature_transform.eval()\n",
    "    \n",
    "    # Results dictionary to store metrics for each noise level and feature type\n",
    "    noise_results = {'raw': {}, 'transformed': {}}\n",
    "    \n",
    "    # Test for each noise level\n",
    "    for noise_level in noise_levels:\n",
    "        print(f\"\\nTesting with noise level: {noise_level}\")\n",
    "        \n",
    "        all_raw_preds = []\n",
    "        all_transformed_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Extract features with noisy images\n",
    "        with torch.no_grad():\n",
    "            for batch_img, batch_reg_img, batch_avg_img, batch_const_img, batch_labels in tqdm(test_loader, \n",
    "                                                                                              desc=f\"Processing (noise={noise_level})\"):\n",
    "                # Add noise to the image\n",
    "                batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2])\n",
    "                noisy_img = add_gaussian_noise(batch_img, noise_level).to(device)\n",
    "                batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], \n",
    "                                                        batch_const_img.shape[2]).to(device)\n",
    "                \n",
    "                # Get features\n",
    "                pred = pretrained_net(noisy_img, batch_const_img, registration=True)\n",
    "                raw_features = pred[2]\n",
    "                transformed_features = feature_transform(raw_features)\n",
    "                \n",
    "                # Flatten features\n",
    "                raw_features = raw_features.view(raw_features.size(0), -1)\n",
    "                transformed_features = transformed_features.view(transformed_features.size(0), -1)\n",
    "                \n",
    "                # Initialize classifiers if first batch\n",
    "                if len(all_raw_preds) == 0:\n",
    "                    raw_dim = raw_features.shape[1]\n",
    "                    transformed_dim = transformed_features.shape[1]\n",
    "                    hidden_dim = 256\n",
    "                    num_classes = len(torch.unique(batch_labels))\n",
    "                    \n",
    "                    # Initialize both classifiers\n",
    "                    raw_classifier = RawFeatureClassifier(raw_dim, hidden_dim, num_classes).to(device)\n",
    "                    transformed_classifier = TransformedFeatureClassifier(transformed_dim, hidden_dim, num_classes).to(device)\n",
    "                    \n",
    "                    # Load states\n",
    "                    raw_classifier.load_state_dict(model_states['raw'])\n",
    "                    transformed_classifier.load_state_dict(model_states['transformed'])\n",
    "                    \n",
    "                    raw_classifier.eval()\n",
    "                    transformed_classifier.eval()\n",
    "                \n",
    "                # Get predictions\n",
    "                raw_outputs = raw_classifier(raw_features.float())\n",
    "                transformed_outputs = transformed_classifier(transformed_features.float())\n",
    "                \n",
    "                _, raw_predicted = torch.max(raw_outputs.data, 1)\n",
    "                _, transformed_predicted = torch.max(transformed_outputs.data, 1)\n",
    "                \n",
    "                all_raw_preds.extend(raw_predicted.cpu().numpy())\n",
    "                all_transformed_preds.extend(transformed_predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.numpy())\n",
    "        \n",
    "        # Calculate metrics for this noise level\n",
    "        raw_metrics = calculate_metrics(np.array(all_labels), np.array(all_raw_preds))\n",
    "        transformed_metrics = calculate_metrics(np.array(all_labels), np.array(all_transformed_preds))\n",
    "        \n",
    "        noise_results['raw'][noise_level] = raw_metrics\n",
    "        noise_results['transformed'][noise_level] = transformed_metrics\n",
    "        \n",
    "        # Print results for this noise level\n",
    "        for feature_type in ['raw', 'transformed']:\n",
    "            metrics = noise_results[feature_type][noise_level]\n",
    "            print(f\"\\nMetrics for {feature_type} features at noise level {noise_level}:\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "            print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "            print(\"\\nPer-class Accuracy:\")\n",
    "            for i, acc in enumerate(metrics['per_class_acc']):\n",
    "                print(f\"Class {i}: {acc:.4f}\")\n",
    "            \n",
    "            # Plot and save confusion matrix\n",
    "            plot_confusion_matrix(\n",
    "                metrics['conf_matrix'], \n",
    "                os.path.join(save_dir, f'confusion_matrix_{feature_type}_noise_{noise_level}.png')\n",
    "            )\n",
    "    \n",
    "    # Plot accuracy vs noise level for both feature types\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    noise_levels_list = list(noise_results['raw'].keys())\n",
    "    \n",
    "    # Plot raw features accuracy\n",
    "    raw_accuracies = [noise_results['raw'][level]['accuracy'] for level in noise_levels_list]\n",
    "    plt.plot(noise_levels_list, raw_accuracies, 'bo-', label='Raw Features')\n",
    "    \n",
    "    # Plot transformed features accuracy\n",
    "    transformed_accuracies = [noise_results['transformed'][level]['accuracy'] for level in noise_levels_list]\n",
    "    plt.plot(noise_levels_list, transformed_accuracies, 'ro-', label='Transformed Features')\n",
    "    \n",
    "    plt.xlabel('Noise Level ()')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy vs. Gaussian Noise Level')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, 'noise_robustness_comparison.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return noise_results\n",
    "\n",
    "# Usage after training both classifiers:\n",
    "print(\"\\nTesting noise robustness...\")\n",
    "model_states = {\n",
    "    'raw': state_raw,\n",
    "    'transformed': state_transformed\n",
    "}\n",
    "\n",
    "noise_results = test_noise_robustness(\n",
    "    pretrained_net=pretrained_net,\n",
    "    feature_transform=feature_transform,\n",
    "    test_loader=test_loader,\n",
    "    model_states=model_states,\n",
    "    noise_levels=[0, 0.001, 0.005, 0.01, 0.05],\n",
    "    save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e6424-3d7a-49f8-8a1d-5bc0d5fa5246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# Feature Extractors\n",
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(self, finetune=False):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        \n",
    "        if not finetune:\n",
    "            for param in self.resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "# Combined Classifiers\n",
    "class CombinedRawClassifier(nn.Module):\n",
    "    def __init__(self, img_dim, shape_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.combined_dim = img_dim + shape_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.combined_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img_features, shape_features):\n",
    "        combined = torch.cat([img_features, shape_features], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "class CombinedTransformedClassifier(nn.Module):\n",
    "    def __init__(self, img_dim, shape_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.combined_dim = img_dim + shape_dim\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.combined_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),  # Lower dropout for transformed features\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.BatchNorm1d(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img_features, shape_features):\n",
    "        combined = torch.cat([img_features, shape_features], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "def extract_features(loader, img_net, shape_net, feature_transform=None, feature_type='raw', device='cuda'):\n",
    "    \"\"\"Extract both image and shape features\"\"\"\n",
    "    img_features_list = []\n",
    "    shape_features_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    img_net.eval()\n",
    "    shape_net.eval()\n",
    "    if feature_transform is not None:\n",
    "        feature_transform.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_img, batch_reg_img, batch_avg_img, batch_const_img, batch_labels in tqdm(loader, desc=\"Extracting features\"):\n",
    "            # Process images\n",
    "            batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2]).to(device)\n",
    "            batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], batch_const_img.shape[2]).to(device)\n",
    "            \n",
    "            # Get image features\n",
    "            img_features = img_net(batch_img)\n",
    "            \n",
    "            # Get shape features\n",
    "            shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "            shape_features = shape_pred[2]\n",
    "            \n",
    "            if feature_type == 'transformed' and feature_transform is not None:\n",
    "                shape_features = feature_transform(shape_features)\n",
    "            \n",
    "            # Flatten shape features\n",
    "            shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "            \n",
    "            img_features_list.append(img_features.cpu())\n",
    "            shape_features_list.append(shape_features.cpu())\n",
    "            labels_list.append(batch_labels)\n",
    "    \n",
    "    img_features = torch.cat(img_features_list, dim=0)\n",
    "    shape_features = torch.cat(shape_features_list, dim=0)\n",
    "    labels = torch.cat(labels_list, dim=0)\n",
    "    \n",
    "    return img_features, shape_features, labels\n",
    "\n",
    "def train_combined_classifier(train_loader, val_loader, img_net, shape_net, feature_transform=None, \n",
    "                            feature_type='raw', finetune_resnet=False, save_dir='./classifier_results'):\n",
    "    \"\"\"Train classifier combining image and shape features\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Extract initial features to get dimensions\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    batch_img = sample_batch[0].reshape(-1, 1, sample_batch[0].shape[1], sample_batch[0].shape[2]).to(device)\n",
    "    batch_const_img = sample_batch[3].reshape(-1, 1, sample_batch[3].shape[1], sample_batch[3].shape[2]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_features = img_net(batch_img)\n",
    "        shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "        shape_features = shape_pred[2]\n",
    "        \n",
    "        if feature_type == 'transformed' and feature_transform is not None:\n",
    "            shape_features = feature_transform(shape_features)\n",
    "        \n",
    "        shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "    \n",
    "    # Get dimensions\n",
    "    img_dim = img_features.shape[1]\n",
    "    shape_dim = shape_features.shape[1]\n",
    "    num_classes = len(torch.unique(sample_batch[-1]))\n",
    "    hidden_dim = 1024 if feature_type == 'raw' else 512  # Larger for raw features\n",
    "    \n",
    "    # Initialize appropriate classifier\n",
    "    if feature_type == 'raw':\n",
    "        model = CombinedRawClassifier(img_dim, shape_dim, hidden_dim, num_classes).to(device)\n",
    "    else:\n",
    "        model = CombinedTransformedClassifier(img_dim, shape_dim, hidden_dim, num_classes).to(device)\n",
    "    \n",
    "    # Create parameter groups\n",
    "    params = []\n",
    "    if finetune_resnet:\n",
    "        params.extend(list(img_net.parameters()))\n",
    "    params.extend(list(model.parameters()))\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params, lr=0.00005, weight_decay=0)\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs = 300\n",
    "    early_stopping_patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_states = None\n",
    "    \n",
    "    # Training history\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    \n",
    "    # Training loop\n",
    "    print(f\"Starting training with {feature_type} shape features...\")\n",
    "    pbar = tqdm(range(num_epochs), desc=\"Training\")\n",
    "    for epoch in pbar:\n",
    "        if finetune_resnet:\n",
    "            img_net.train()\n",
    "        model.train()\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # Training\n",
    "        for batch_img, _, _, batch_const_img, batch_labels in train_loader:\n",
    "            batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2]).to(device)\n",
    "            batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], batch_const_img.shape[2]).to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            # Get features\n",
    "            img_features = img_net(batch_img)\n",
    "            shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "            shape_features = shape_pred[2]\n",
    "            \n",
    "            if feature_type == 'transformed' and feature_transform is not None:\n",
    "                shape_features = feature_transform(shape_features)\n",
    "            \n",
    "            shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(img_features, shape_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += batch_labels.size(0)\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        img_net.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_img, _, _, batch_const_img, batch_labels in val_loader:\n",
    "                batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2]).to(device)\n",
    "                batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], batch_const_img.shape[2]).to(device)\n",
    "                batch_labels = batch_labels.to(device)\n",
    "                \n",
    "                # Get features\n",
    "                img_features = img_net(batch_img)\n",
    "                shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "                shape_features = shape_pred[2]\n",
    "                \n",
    "                if feature_type == 'transformed' and feature_transform is not None:\n",
    "                    shape_features = feature_transform(shape_features)\n",
    "                \n",
    "                shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "                \n",
    "                outputs = model(img_features, shape_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        train_acc = train_correct / total_train\n",
    "        val_acc = val_correct / total_val\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'val_loss': f'{val_loss:.4f}',\n",
    "            'train_acc': f'{train_acc:.4f}',\n",
    "            'val_acc': f'{val_acc:.4f}'\n",
    "        })\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_states = {\n",
    "                'classifier': model.state_dict(),\n",
    "                'img_net': img_net.state_dict() if finetune_resnet else None\n",
    "            }\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Save best model\n",
    "    torch.save(best_model_states, os.path.join(save_dir, f'best_combined_classifier_{feature_type}.pth'))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.title(f'Loss History ({feature_type})')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['val_acc'], label='Val Acc')\n",
    "    plt.title(f'Accuracy History ({feature_type})')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'training_history_{feature_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return model, img_net, best_model_states\n",
    "\n",
    "def test_combined_classifier(test_loader, img_net, shape_net, feature_transform=None, \n",
    "                           feature_type='raw', model_states=None, save_dir='./classifier_results'):\n",
    "    \"\"\"Test the combined classifier\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize networks\n",
    "    if model_states['img_net'] is not None:\n",
    "        img_net.load_state_dict(model_states['img_net'])\n",
    "    img_net.eval()\n",
    "    shape_net.eval()\n",
    "    if feature_transform is not None:\n",
    "        feature_transform.eval()\n",
    "    \n",
    "    # Get dimensions from a single batch\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    batch_img = sample_batch[0].reshape(-1, 1, sample_batch[0].shape[1], sample_batch[0].shape[2]).to(device)\n",
    "    batch_const_img = sample_batch[3].reshape(-1, 1, sample_batch[3].shape[1], sample_batch[3].shape[2]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_features = img_net(batch_img)\n",
    "        shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "        shape_features = shape_pred[2]\n",
    "        \n",
    "        if feature_type == 'transformed' and feature_transform is not None:\n",
    "            shape_features = feature_transform(shape_features)\n",
    "        \n",
    "        shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "    \n",
    "    # Get dimensions\n",
    "    img_dim = img_features.shape[1]\n",
    "    shape_dim = shape_features.shape[1]\n",
    "    num_classes = len(torch.unique(sample_batch[-1]))\n",
    "    hidden_dim = 1024 if feature_type == 'raw' else 512\n",
    "    \n",
    "    # Initialize appropriate classifier\n",
    "    # Initialize appropriate classifier\n",
    "    if feature_type == 'raw':\n",
    "        model = CombinedRawClassifier(img_dim, shape_dim, hidden_dim, num_classes).to(device)\n",
    "    else:\n",
    "        model = CombinedTransformedClassifier(img_dim, shape_dim, hidden_dim, num_classes).to(device)\n",
    "    \n",
    "    model.load_state_dict(model_states['classifier'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Testing\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_img, _, _, batch_const_img, batch_labels in tqdm(test_loader, desc=f\"Testing ({feature_type})\"):\n",
    "            batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2]).to(device)\n",
    "            batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], batch_const_img.shape[2]).to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            img_features = img_net(batch_img)\n",
    "            shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "            shape_features = shape_pred[2]\n",
    "            \n",
    "            if feature_type == 'transformed' and feature_transform is not None:\n",
    "                shape_features = feature_transform(shape_features)\n",
    "            \n",
    "            shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "            \n",
    "            # Get predictions\n",
    "            outputs = model(img_features, shape_features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    metrics = calculate_metrics(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"\\nTest Set Metrics ({feature_type}):\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(\"\\nPer-class Accuracy:\")\n",
    "    for i, acc in enumerate(metrics['per_class_acc']):\n",
    "        print(f\"Class {i}: {acc:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(metrics['conf_matrix'], \n",
    "                        os.path.join(save_dir, f'confusion_matrix_{feature_type}.png'))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def test_noise_robustness(test_loader, img_net, shape_net, feature_transform=None, \n",
    "                         feature_type='raw', model_states=None, \n",
    "                         noise_levels=[0, 0.001, 0.005, 0.01, 0.05], \n",
    "                         save_dir='./classifier_results'):\n",
    "    \"\"\"Test model robustness against different levels of Gaussian noise\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize networks\n",
    "    if model_states['img_net'] is not None:\n",
    "        img_net.load_state_dict(model_states['img_net'])\n",
    "    img_net.eval()\n",
    "    shape_net.eval()\n",
    "    if feature_transform is not None:\n",
    "        feature_transform.eval()\n",
    "    \n",
    "    # Get dimensions from a single batch\n",
    "    sample_batch = next(iter(test_loader))\n",
    "    batch_img = sample_batch[0].reshape(-1, 1, sample_batch[0].shape[1], sample_batch[0].shape[2]).to(device)\n",
    "    batch_const_img = sample_batch[3].reshape(-1, 1, sample_batch[3].shape[1], sample_batch[3].shape[2]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_features = img_net(batch_img)\n",
    "        shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "        shape_features = shape_pred[2]\n",
    "        \n",
    "        if feature_type == 'transformed' and feature_transform is not None:\n",
    "            shape_features = feature_transform(shape_features)\n",
    "        \n",
    "        shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "    \n",
    "    # Get dimensions\n",
    "    img_dim = img_features.shape[1]\n",
    "    shape_dim = shape_features.shape[1]\n",
    "    num_classes = len(torch.unique(sample_batch[-1]))\n",
    "    hidden_dim = 1024 if feature_type == 'raw' else 512\n",
    "    \n",
    "    # Initialize appropriate classifier\n",
    "    if feature_type == 'raw':\n",
    "        model = CombinedRawClassifier(img_dim, shape_dim, hidden_dim, num_classes).to(device)\n",
    "    else:\n",
    "        model = CombinedTransformedClassifier(img_dim, shape_dim, hidden_dim, num_classes).to(device)\n",
    "    \n",
    "    model.load_state_dict(model_states['classifier'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Results dictionary to store metrics for each noise level\n",
    "    noise_results = {}\n",
    "    \n",
    "    # Test for each noise level\n",
    "    for noise_level in noise_levels:\n",
    "        print(f\"\\nTesting with noise level: {noise_level}\")\n",
    "        \n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # Test with noisy images\n",
    "        with torch.no_grad():\n",
    "            for batch_img, _, _, batch_const_img, batch_labels in tqdm(test_loader, desc=f\"Testing noise={noise_level}\"):\n",
    "                # Add noise to the images\n",
    "                batch_img = batch_img.reshape(batch_img.shape[0], 1, batch_img.shape[1], batch_img.shape[2])\n",
    "                batch_img = add_gaussian_noise(batch_img, noise_level)\n",
    "                batch_img = batch_img.to(device)\n",
    "                \n",
    "                batch_const_img = batch_const_img.reshape(batch_const_img.shape[0], 1, batch_const_img.shape[1], batch_const_img.shape[2]).to(device)\n",
    "                \n",
    "                # Extract features\n",
    "                img_features = img_net(batch_img)\n",
    "                shape_pred = shape_net(batch_img, batch_const_img, registration=True)\n",
    "                shape_features = shape_pred[2]\n",
    "                \n",
    "                if feature_type == 'transformed' and feature_transform is not None:\n",
    "                    shape_features = feature_transform(shape_features)\n",
    "                \n",
    "                shape_features = shape_features.view(shape_features.size(0), -1)\n",
    "                \n",
    "                # Get predictions\n",
    "                outputs = model(img_features, shape_features)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.numpy())\n",
    "        \n",
    "        # Calculate metrics for this noise level\n",
    "        metrics = calculate_metrics(np.array(all_labels), np.array(all_preds))\n",
    "        noise_results[noise_level] = metrics\n",
    "        \n",
    "        # Print results for this noise level\n",
    "        print(f\"\\nMetrics for noise level {noise_level}:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(\"\\nPer-class Accuracy:\")\n",
    "        for i, acc in enumerate(metrics['per_class_acc']):\n",
    "            print(f\"Class {i}: {acc:.4f}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        plot_confusion_matrix(\n",
    "            metrics['conf_matrix'], \n",
    "            os.path.join(save_dir, f'confusion_matrix_{feature_type}_noise_{noise_level}.png')\n",
    "        )\n",
    "    \n",
    "    # Plot accuracy vs noise level\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    noise_levels_list = list(noise_results.keys())\n",
    "    accuracies = [noise_results[level]['accuracy'] for level in noise_levels_list]\n",
    "    \n",
    "    plt.plot(noise_levels_list, accuracies, 'bo-')\n",
    "    plt.xlabel('Noise Level ()')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Model Accuracy vs. Gaussian Noise Level ({feature_type})')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(save_dir, f'noise_robustness_{feature_type}.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return noise_results\n",
    "\n",
    "# Utility functions\n",
    "def add_gaussian_noise(images, noise_level):\n",
    "    \"\"\"Add Gaussian noise to images\"\"\"\n",
    "    noise = torch.randn_like(images) * noise_level\n",
    "    noisy_images = images + noise\n",
    "    return torch.clamp(noisy_images, 0, 1)\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate all metrics\"\"\"\n",
    "    accuracy = (y_true == y_pred).mean()\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    per_class_acc = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'per_class_acc': per_class_acc,\n",
    "        'conf_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(conf_matrix, save_path=None):\n",
    "    \"\"\"Plot and optionally save confusion matrix\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Load models\n",
    "    pretrained_net = torch.load('...').to(device)\n",
    "    feature_transform = torch.load('...').to(device)\n",
    "    img_net = ImageFeatureExtractor(finetune=True).to(device)\n",
    "    \n",
    "    save_dir = './saved_models'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Train and test with raw shape features\n",
    "    print(\"\\nTraining with raw shape features...\")\n",
    "    model_raw, img_net_raw, states_raw = train_combined_classifier(\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        img_net=img_net,\n",
    "        shape_net=pretrained_net,\n",
    "        feature_transform=None,\n",
    "        feature_type='raw',\n",
    "        finetune_resnet=True,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTesting with raw shape features...\")\n",
    "    metrics_raw = test_combined_classifier(\n",
    "        test_loader,\n",
    "        img_net=img_net_raw,\n",
    "        shape_net=pretrained_net,\n",
    "        feature_transform=None,\n",
    "        feature_type='raw',\n",
    "        model_states=states_raw,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTesting noise robustness with raw shape features...\")\n",
    "    noise_results_raw = test_noise_robustness(\n",
    "        test_loader,\n",
    "        img_net=img_net_raw,\n",
    "        shape_net=pretrained_net,\n",
    "        feature_transform=None,\n",
    "        feature_type='raw',\n",
    "        model_states=states_raw,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    # Train and test with transformed shape features\n",
    "    print(\"\\nTraining with transformed shape features...\")\n",
    "    model_transformed, img_net_transformed, states_transformed = train_combined_classifier(\n",
    "        train_loader, \n",
    "        val_loader,\n",
    "        img_net=img_net,\n",
    "        shape_net=pretrained_net,\n",
    "        feature_transform=feature_transform,\n",
    "        feature_type='transformed',\n",
    "        finetune_resnet=True,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTesting with transformed shape features...\")\n",
    "    metrics_transformed = test_combined_classifier(\n",
    "        test_loader,\n",
    "        img_net=img_net_transformed,\n",
    "        shape_net=pretrained_net,\n",
    "        feature_transform=feature_transform,\n",
    "        feature_type='transformed',\n",
    "        model_states=states_transformed,\n",
    "        save_dir=save_dir\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTesting noise robustness with transformed shape features...\")\n",
    "    noise_results_transformed = test_noise_robustness(\n",
    "        test_loader,\n",
    "        img_net=img_net_transformed,\n",
    "        shape_net=pretrained_net,\n",
    "        feature_transform=feature_transform,\n",
    "        feature_type='transformed',\n",
    "        model_states=states_transformed,\n",
    "        save_dir=save_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6d6111-8aeb-4083-9bac-e11c299a5503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
